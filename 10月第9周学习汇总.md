# 2022年10月第9周学习汇总——宋世淼

## 深度学习

- 学习流程

  - 数据获取
  - 特征工程（**最核心**）
    - 数据特征决定了模型的上限
    - 预处理和特征提取是最核心的
    - 算法与参数选择决定了如何逼近这个上限
  - 建立模型
  - 评估与应用

- 计算机视觉：

  - 图像分类任务

- 计算机视觉面临的挑战：

  照射角度、形状改变、部分遮蔽、背景混入



### 神经网络基础

- **线性函数**（得分函数）：从输入 $\rightarrow$ 输出的映射(10分类)
  $$
  \ce{image(32\times32\times3) ->[f(x,W)_{10\times1}=W_{10\times3070}\cdot x_{3070\times1}(+b_{10\times1})]}\ce{每个类别的得分}
  $$
  多组权重参数构成了决策边界
  $$
  f(x_i, W, b)=Wx_i+b
  $$

- **损失函数**：结果的得分值有着明显的差异，我们需要明确得知道模型得当前效果，有多好或是多差。

  做不同的任务，就是损失函数得区别。
  $$
  损失函数 = 数据损失 + 正则化惩罚项
  $$

  $$
  L = \frac{1}{N}\sum_{i=1}^{N}\sum_{j\ne y_i}max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)+\lambda R(W)
  $$

  正则化惩罚项：$R(W)=\sum_k\sum_lW^2_{k,l}$ （我们总是希望模型不要太复杂，过拟合得模型是没有用的）

- **Softmax**分类器

  将得到的一个输入的得分值，转换成一个概率值$g(z)=\frac{1}{1+e^{-z}}$ (sigmoid函数)

  softmax则为：
  $$
  \ce{归一化：}P(Y=k|X=x_i)=\frac{e^{s_k}}{\sum_je^{s_j}},\  \ce{where}\ (s=f(x_i;W))
  $$

  $$
  \ce{计算损失值：}L_i=-logP(Y=y_i|X=x_i)
  $$

- - **前向传播**：得出损失值

    ```mermaid
    graph LR
    x --> A((*))
    W --> A((*))
    A((*)) -- scores --> B((loss)) --> C((+)) --> L
    W --> D((R)) --> C
    ```

  - **反向传播**：更新模型（梯度下降）

    计算梯度时，是逐层计算。（链式法则：梯度是一步一步传的）



### 神经网络整体架构

- 层次结构(Layer)，神经元(数据的量，矩阵的大小)，全连接，非线性

- 线性方程：$f=Wx$

- 非线性方程：$f=W_2\cdot max(0,W_1x)$ 

- 计算结果：$\ce{x_{3072} ->[W_1]h_100->[W_2]s_10}$ 

- 基本结构：$f=W_2\cdot NonlinearFunc(W_1x)$ 

  继续堆叠一层：$f=W_3\cdot NF(W_2\cdot NF(W_1x))$

  神经网络的强大之处在于，用更多的参数来拟合复杂的数据。

- 正则化的作用：防止**过拟合**

- 神经元：个数对结果的影响，数量越多，过拟合风险越大

- **激活函数**(非常重要的一部分)：常用的激活函数(Sigmoid, Relu, Tanh 等)

- 数据预处理：不同的预处理结果会使得模型的效果发生很大差异

- **参数初始化**：同样非常重要，通常用随机策略来进行参数初始化`W = 0.01 * np.random.randn(D, H)` 

- **Dropout**：过拟合是神经网络非常头疼的大问题。在神经网络训练的过程中，随机地杀死一部分神经元



## 卷积神经网络

- 卷积神经网络可以做什么任务：

  检测任务，分类与检索，超分辨率重构，医学任务，字体，标识识别，无人驾驶，人脸识别等

- **整体架构**：输入层，卷积层（提取特征），池化层（压缩特征），全连接层

  - 卷积做了一件什么事：利用卷积核提取图像的特征，得到**特征图**。

  - 图像颜色通道：RGB等颜色空间

  - 特征图个数：几个卷积核，就有几张特征图（每张图，每个通道）

  - 只做一次卷积可以吗？卷积层堆叠

    ```mermaid
    graph LR
    A[Image] --> B[Low-Level Feature]
    B --> C[Mid-Level Feature]
    C --> D[High-Level Feature]
    D --> E[Trainable Classifier]
    ```

    多次卷积，提取不同层次的特征

  - **卷积层涉及参数：滑动窗口步长，卷积核尺寸，边缘填充，卷积核个数**

  - **卷积结果计算公式**：

    - 长度：$H_2=\frac{H_1-F_H+2P}{S}+1$
    - 宽度：$W_2=\frac{W_1-F_W+2P}{S}+1$

    其中$W_1,H_1$ 表示输入的宽度，长度；$W_2,H_2$ 表示输出特征图的宽度，长度；$F$ 表示卷积核长和宽的大小；$S$ 表示滑动窗口的步长；$P$ 表示边界填充（加了几圈0）。

  - 卷积参数共享

  - **池化层**：压缩/下采样（通过某种方法）例如：maxpooling最大池化

  - 感受野：

    如果堆叠3个3×3的卷积层，并且保持滑动窗口步长为1，其感受野就是7×7的了，这跟一个使用7×7卷积核的结果是一样的。

    假设输入大小都是$h\times w\times c$，并且都使用$c$ 个卷积核（得到$c$ 个特征图），计算一下各自所需参数：

    - 一个$7\times7$卷积核所需参数$=c\times(7\times7\times c)=49c^2$ 
    - 三个$3\times3$卷积核所需参数$=3\times c\times(3\times3\times c)=27c^2$ 

    很明显，**堆叠小的卷积核所需的参数更少一些，并且卷积过程越多，特征提取也会越细致，加入的非线性变换也随着增多，还不会增大权重参数个数**。
